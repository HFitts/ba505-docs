# IS505 Final Projects
__Fall 2017__

## Overview
Students work in teams of 3 students to define and complete a moderately-sized analytical project in Python. Detailed instructions are provided regarding selecting and framing a suitable analysis. Grading criteria will reflect demonstrated achievement of the objectives and compliance with the requirements.

## Objectives
* Apply Python skills to a student-selected analytical project of moderate (but nontrivial) scope and complexity.
* Demonstrate mastery of Python, NumPy, Pandas, Matplolib, Jupyter, and GitHub.
* Work in teams, with professional-quality tools and standards.

## Project Requirements
* **All work must be your own.** It is your responsibility to demonstrate the unique contributions made by your work.
* **Academic honesty policies and scholarship norms apply.** All relevant previous work and data sources must be properly cited.
* **The analysis must be fully reproducible, even and especially with new data.** This is to be professional work, subject to prevalent standards. Suggestion: use unit tests to verify that the data is loading correctly and that the statistical measures are doing what you expect.
* **Descriptive analytics (as in QA400) are sufficient.** However, there is no need to constrain your work. If you can apply forecasting or decision modeling, then feel free to do so.
* **The analysis should stand on its own.** All work will be posted to GitHub Classroom. You will be asked to present your work on the final day of class, but the analysis itself should be readily accessible and understandable from your GitHub repositories without installing any additional software.
* **Project size and scope should be sufficient to demonstrate mastery of Python, NumPy, Pandas, and Matplotlib.** At a minimum the datasets used should have at least 10K records with a dozen or more fields. Further, there should be some opportunity for you to filter/reduce/analyze the data using Pandas and NumPy. If the work could just as easily be performed by an undergraduate in Excel, then keep looking for another project.
* **Only datasets from public sources are allowed.** It should be possible for anyone to reproduce your work, including collecting the data from original sources.
* **Document your work.** At a minimum, your Jupyter notebooks must document the scope, purpose, and significance of your work. Each analytical step (code cell) should be documented with markdown cells above and below stating your intention (above) and interpretation (below). Your notebooks should tell a story about the data. **One should be able to read the rendered notebook in GitHub and know exactly what you did, why you did it, and why we should care about the results.**

## Instructions  

1. **Watch and learn.** Study the following tutorial, which sets the bar for what constitutes A-level work in this course:  
[Reproducible Data Analysis in Jupyter](https://jakevdp.github.io/blog/2017/03/03/reproducible-data-analysis-in-jupyter)  
The tutorial is in 10 parts, with some of the more advanced work in the second half. At a minimum, your project should use the practices in videos 1-5. To get the best grade, however, try to use one or more of the practices shown in videos 6-10.  
2. **Survey the best examples you can find.** One very convenient place to look is [Kaggle](https://kaggle.com), which has submissions from numerous data analytics competitions. Here a few particularly engaging Kaggle submissions to study:
  * [asindico//from-exploration-to-prediction](https://www.kaggle.com/asindico/from-exploration-to-prediction)
  * [anokas/exploratory-data-analysis-4 2017](https://www.kaggle.com/anokas/exploratory-data-analysis-4)
  * [sudalairajkumar/simple-exploration-notebook-5](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-5)
  * [pmarcelino/comprehensive-data-exploration-with-python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)

   While some are more "finished" than others, you'll find that each tells a story with data. Try to learn to do that.  

3. **Find a suitable data source.** It must meet the requirements above. If you are using a dataset discovered through Kaggle, then be sure to cite it and state how the analysis you are doing is different. Whatever you do, don't just replicate someone else's analysis. That will result in an immediate F for the course and expulsion from the program.
4. **Develop a few research questions that you'd like to answer based on the data.** The questions should be stated in the preamble to your notebook. The analysis should also demonstrate your abilities. Did you learn what you needed to from this course? Read the requirements again to makes sure your analysis will comply.
5. **Do some analysis**. This is likely to be a learn-as-you-go process. Suggestion: each team member should keep a scratch notebook file for trying things out. Then transfer your work to the finished notebook just before completion. However, do not commit your scratch notebooks or checkpoint files to GitHub. (Use a `.gitignore` file for that.)
6. **Try something more advanced.** Once you have a basic framework for your analysis, extend it with something more advanced. Perhaps you could try an advanced plot or maybe another data source or maybe replicated sampling to demonstrate reproducibility/lack of bias.
7. **Clean up your notebook and GitHub repo.** The notebook is your presentation (i.e., there will be no slides) so make sure it is readable on a TV monitor from across the room. It is also your report, so make sure every objective, decision, interpretation, and conclusion you make is throughly explained and professional-looking. (Yes, styling and organization matters. We're professionals, right?) Make sure you don't have any unfinished or extraneous artifacts left over from your explorations. (Again, we're professionals, not undergrads.)
